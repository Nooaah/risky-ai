# Risky Game (Team Blue)

![](resources/blue_logo.png)

### üîµ The Team Blue

- Development of the Q-learning AI:
  * **Pierre Calon** - ([Github](https://github.com/PierreCalon))
    - Q-Learning Lead Developer
  * **Noah Ch√¢telain** - ([Github](https://github.com/Nooaah))
    - Secondary Q-Learning Developer
    - Git Master
- Development of the Monte-Carlo and optimization:
  * **Tom Maillard** - ([Github](https://github.com/wipcamael))
    - Monte-Carlo Lead Developer
  * **Marvin Bonnet** - ([Github](https://github.com/AastroLePetitRobot))
    - Secondary Monte-Carlo Developer
#### Contributors

- Permanent contributor:
  * **Guillaume LOZENGUEZ** - [guillaume.lozenguez@imt-nord-europe.fr](mailto:guillaume.lozenguez@imt-nord-europe.fr)

### Quelques explications de notre projet
___
#### Premi√®re version

Notre premi√®re version du Risky √©tait bas√©e sur un Q-Learning.
- Derni√®re version disponible au commit [6e91f01](https://github.com/Nooaah/risky-ai/tree/6e91f01a3da53ff941a874350bd369a5c05ae89b)

Nous avons d√©cid√© de remplir le JSON de notre Q-Learning avec tout l'√©tat du board actuel, afin de pouvoir jouer des millions d'entra√Ænements par la suite, et d'avoir un Q-Learning tr√®s performant. Mais le nombre d'√©tats possibles √©tait beaucoup trop grand, et le temps d'entra√Ænement de notre Q-Learning aurait √©t√© beaucoup trop long. Nous avons donc d√©cid√© par la suite de r√©duire les nos diff√©rents √©tats, afin de r√©duire la taille de notre JSON.

#### Ajout d'un algorithme de Monte-Carlo

Nous avons ensuite d√©cid√© de cr√©er un Monte-Carlo, qui se chargera par la suite d'ins√©rer des valeurs correctes dans le JSON de notre Q-Learning.
- Derni√®re version du MCTS disponible sur la branche [mcts-latest](https://github.com/Nooaah/risky-ai/tree/mcts-latest)

#### R√©sultats

Les r√©sultats de notre Monte-Carlo n'√©tant pas les r√©sultats attendus, le temps des entra√Ænements √©taient beaucoup trop longs (Environ 5 secondes par parties) et il √©tait donc quasiement impossible de remplir le JSON de notre Q-Learning, car les valeurs de nos diff√©rents √©tats r√©apparaissent tr√®s rarement, voire jamais.

#### Dernier recours

Notre projet, avec nos algorithmes de Q-Learning et de Monte-Carlo, √©tant tous deux fonctionnels, mais n'apportant pas de r√©sultats assez satisfaisants lors du dernier jour du projet, nous avons donc d√©cid√© de r√©aliser notre propre algorithme de derni√®re minute, qui suit notre propre strat√©gie de jeu, afin d'avoir au moins une IA fonctionnelle, pouvant battre l'IA random, l'IA prof, et quelques IA des autres teams.
- Derni√®re version du projet sur la branche [main](https://github.com/Nooaah/risky-ai/tree/main)
